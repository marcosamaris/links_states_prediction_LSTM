{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T02:32:33.519551Z",
     "start_time": "2019-10-09T02:32:31.946220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2052, 27)\n",
      "(180, 27)\n",
      "(2052, 1, 27, 1)\n",
      "(180, 1, 27, 1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Apr  4 14:53:08 2019\n",
    "\n",
    "@author: marcos\n",
    "\"\"\"\n",
    "\n",
    "'''Example of VAE on MNIST dataset using CNN\n",
    "\n",
    "The VAE has a modular design. The encoder, decoder and VAE\n",
    "are 3 models that share weights. After training the VAE model,\n",
    "the encoder can be used to  generate latent vectors.\n",
    "The decoder can be used to generate MNIST digits by sampling the\n",
    "latent vector from a Gaussian distribution with mean=0 and std=1.\n",
    "\n",
    "# Reference\n",
    "\n",
    "[1] Kingma, Diederik P., and Max Welling.\n",
    "\"Auto-encoding variational bayes.\"\n",
    "https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Conv2D, Flatten, Lambda\n",
    "from keras.layers import Reshape, Conv2DTranspose, Cropping2D\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# then z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "selected_line =['8700-10-1',\n",
    "                '7545-10-1',\n",
    "                '7545-10-0',   #Less two links\n",
    "                '6450-10-1',\n",
    "                '6450-10-0',\n",
    "                '3301-10-1',   #Less two links\n",
    "                '2290-10-1',\n",
    "                '2290-10-0',\n",
    "                '477P-10-0',        \n",
    "                '3301-10-0',   #Less two links\n",
    "                '574J-10-1',   #Less two links\n",
    "                '574J-10-0',   #Less two links\n",
    "                '477P-10-1',   #Less two links\n",
    "                '351F-10-1',\n",
    "                '351F-10-0'] \n",
    "\n",
    "selected_line = ['6450-10-0']\n",
    "\n",
    "### Size of the steps to group\n",
    "frequencies = ['20min', '30min', '1H', '3H', '1d', '7d', '1m']\n",
    "frequencies = ['30min']\n",
    "\n",
    "\n",
    "for line in selected_line:  \n",
    "    filename = './data_temp/' + str(line) + '_temp.csv.gz'\n",
    "    df = pd.read_csv(filename, compression='gzip', sep=',')\n",
    "    df['exact_time'] = pd.to_datetime(df['exact_time'], format = '%Y-%m-%d %H:%M')\n",
    "    df.index = df['exact_time']\n",
    "    \n",
    "    df.loc[(df['time_link'] > 5),'time_link'] = np.ceil(df['time_link'].mean())\n",
    "    \n",
    "    start_date = pd.to_datetime('2017-1-1', format = '%Y-%m-%d')\n",
    "    end_date = pd.to_datetime('2017-9-25', format = '%Y-%m-%d')\n",
    "    df = df.loc[(df['holiday'] != 1) & ((df['weekday'] > 0) & (df['weekday'] < 5))]\n",
    " \n",
    "    \n",
    "    frequency = '60min'            \n",
    "    rolling_win = 1\n",
    "    df = df.drop(df[df['link'] == max(df['link'])].index)\n",
    "    if (line == '7545-10-1') | (line == '477P-10-1') | (line == '3301-10-0') | \\\n",
    "            (line == '3301-10-0') | (line == '574J-10-1') | (line == '574J-10-1'):\n",
    "            df = df.drop(df[df['link'] == max(df['link'])].index)\n",
    "\n",
    "    X_Temp = df.groupby([pd.Grouper(freq=str(frequency)), 'link'], as_index=True ).mean()['time_link'].unstack()    \n",
    "    X_Temp = X_Temp.transform(lambda x: x.fillna(method='ffill')).dropna()\n",
    "    \n",
    "    X_Temp = X_Temp.iloc[X_Temp.index.indexer_between_time('06:00', '23:00')]\n",
    "    \n",
    "    X_Temp.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    X_Temp['exact_time'] = pd.to_datetime(X_Temp['exact_time']).dt.date\n",
    "\n",
    "    result = X_Temp.groupby('exact_time').count()[0]\n",
    "\n",
    "    var = result.loc[result == result.max()].index\n",
    "\n",
    "    X_Temp.index = X_Temp['exact_time']\n",
    "    del X_Temp['exact_time']\n",
    "\n",
    "    X_Temp = X_Temp.loc[pd.to_datetime(list(var))] \n",
    "    \n",
    "    # this is the size of our encoded representations\n",
    "    encoding_dim = 5  \n",
    "    input_dim_x = 3\n",
    "    input_dim_y = 27\n",
    "    \n",
    "    number_test_samples = 10\n",
    "    # Creates the train and test sets \n",
    "    test_samples = result.max()*number_test_samples\n",
    "    train = X_Temp.values[:-test_samples,:-2] # two last columns have large errors\n",
    "    test = X_Temp.values[-test_samples:,:-2] # two last columns have large errors\n",
    "\n",
    "    #Normalize the inputs\n",
    "    print(np.shape(train))\n",
    "    print(np.shape(test))\n",
    "    trmin = train.min(); trmax = train.max();\n",
    "    temin = test.min(); temax = test.max();\n",
    "    train_norm = (train-trmin)/(trmax-trmin)\n",
    "    test_norm  = (test-temin)/(temax-temin)\n",
    "    \n",
    "    train_norm = np.reshape(train_norm, (int(np.shape(train_norm)[0]),int(result.max()/18), np.shape(train_norm)[1], 1))\n",
    "    test_norm = np.reshape(test_norm, (int(np.shape(test_norm)[0]),int(result.max()/18), np.shape(test_norm)[1], 1))\n",
    "    print(np.shape(train_norm))\n",
    "    print(np.shape(test_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
